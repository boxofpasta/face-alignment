Average euclidean distance squared b/w pred and truth points. 
Note that all point coordinates (there are 194) were normalized to be b/w [0, 1].

fully_connected_v1.h5 : 0.00011050
fully_connected_v2.h5 (fine-tuned on 1/4 of the dataset) : 0.00011460

fully_connected_sparse_025 : 0.0340840682392
fully_connected_sparse_100 : 0.0321483014775

fully_connected_025 : 0.0415233714881
fully_connected_100 : 

what if more reference points helps convergence?

lip_masker_030_zoomed_v1 train error with 128 depth transposed convs (360 epochs): 17686.7696
lip_masker_030_zoomed_v2 train error with 256 depth transposed convs (240 epochs): 17722.7897

lip_masker_030_zoomed train error for 28x28 (240 epochs): 12859.6017
lip_masker_030_zoomed train error for 56x56 (240 epochs): ~49500

lip_masker_sep_050 train error for 56x56 (240 epochs @ 8E-4): ~92500 

lip_masker_100 train error for 56x56 (340 epochs @ 8E-3): ~90800

when using randomized bboxes in training:
lip_masker_rand_bboxes_100 for 56x56 (480 epochs @ 1E-4): ~91000

point maskers:

losses are put in a list. last one is typically the per-coord softmax loss. 
before that are sigmoid losses. first one has lowest level features (less meaningful).
rcf (70 epochs @ 1E-3) : [164366, 189204, 4863]

# ------------------------------
# Week of February 11:
# ------------------------------
small point masker:
(100 epochs @ 1E-3) : loss: 212788.3571 - f0_loss: 55532.1652 - f1_loss: 60587.4566 - f2_loss: 83933.9861 - f3_loss: 12734.7497

for 14x14 resolution, lowest resolution loss:
    guessing all 0s:
    52170.3648

    fairly decent results:
    ~49900 

# stddevs : [0.008, 0.02, 0.03, 0.05]
(120 epochs @ 3E-3) : loss: 76760.3013 - f0_loss: 0.0000e+00 - f1_loss: 25798.5723 - f2_loss: 38718.6256 - f3_loss: 12243.1034

# after adding stop_gradient for the conditioning masks from lower-res layers
# stddevs : [0.008, 0.015, 0.03, 0.05]
(120 epochs @ 3E-3) : loss: 44274.7353 - f0_loss: 0.0000e+00 - f1_loss: 36356.2732 - f2_loss: 5704.4272 - f3_loss: 2214.0351

# this sample is problematic (left eye identifies as a lip)
1198061571_1

# vanilla point masker
# stddev for 56x56 : 0.02
(180 epochs? @ 5E-3) : loss: 5477.5687 - f0_loss: 5477.5687 - val_loss: 8123.8908 - val_f0_loss: 8123.8908

# fancy point masker

# vanilla point masker, no skip connections
(160 epochs? @ 1E-2) : 142ms/step - loss: 5903.6019 - f0_loss: 5903.6019 - val_loss: 8774.8352 - val_f0_loss: 8774.8352

# removed 256 deep layer, changed kernel size from 3x3 to 7x7 for 14x14 feature map
(260 epochs @ 5E-3) : loss: 5239.6429 - f0_loss: 5239.6429 - val_loss: 7949.9180 - val_f0_loss: 7949.9180
2728400620_1 -- really bad on left side of lip
2139669544_1 -- right part of lip is in the dark, tough one
1037255513_1 -- really bad on left side of lip
2426631349_1 -- epic fail on top side of lip

# same model but trained on x-flipped data
(350 epochs @ 5E-3) : loss: 5301.4107 - f0_loss: 5301.4107 - val_loss: 8046.8749 - val_f0_loss: 8046.8749

# dilated convolutions model @ commit c183f349bbaf1d28ce952ab8e22fd1f9189f58c0
(140 epochs? @ 5E-3) : loss: 5713.1293 - f0_loss: 5713.1293 - val_loss: 7471.6771 - val_f0_loss: 7471.6771

# without the final 16-dilated layer and 1 extra conv layer at 112 resolution
(150 epochs? @ 1E-2) : loss: 5828.1249 - f0_loss: 5828.1249 - val_loss: 7757.0386 - val_f0_loss: 7757.0386