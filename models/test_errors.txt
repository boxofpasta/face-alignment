Average euclidean distance squared b/w pred and truth points. 
Note that all point coordinates (there are 194) were normalized to be b/w [0, 1].

fully_connected_v1.h5 : 0.00011050
fully_connected_v2.h5 (fine-tuned on 1/4 of the dataset) : 0.00011460

fully_connected_sparse_025 : 0.0340840682392
fully_connected_sparse_100 : 0.0321483014775

fully_connected_025 : 0.0415233714881
fully_connected_100 : 

what if more reference points helps convergence?

lip_masker_030_zoomed_v1 train error with 128 depth transposed convs (360 epochs): 17686.7696
lip_masker_030_zoomed_v2 train error with 256 depth transposed convs (240 epochs): 17722.7897

lip_masker_030_zoomed train error for 28x28 (240 epochs): 12859.6017
lip_masker_030_zoomed train error for 56x56 (240 epochs): ~49500

lip_masker_sep_050 train error for 56x56 (240 epochs @ 8E-4): ~92500 

lip_masker_100 train error for 56x56 (340 epochs @ 8E-3): ~90800

when using randomized bboxes in training:
lip_masker_rand_bboxes_100 for 56x56 (480 epochs @ 1E-4): ~91000

point maskers:

losses are put in a list. last one is typically the per-coord softmax loss. 
before that are sigmoid losses. first one has lowest level features (less meaningful).
rcf (70 epochs @ 1E-3) : [164366, 189204, 4863]

small point masker:
(100 epochs @ 1E-3) : loss: 212788.3571 - f0_loss: 55532.1652 - f1_loss: 60587.4566 - f2_loss: 83933.9861 - f3_loss: 12734.7497

for 14x14 resolution, lowest resolution loss:
    guessing all 0s:
    52170.3648

    fairly decent results:
    ~49900 

# stddevs : [0.008, 0.02, 0.03, 0.05]
(120 epochs @ 3E-3) : loss: 76760.3013 - f0_loss: 0.0000e+00 - f1_loss: 25798.5723 - f2_loss: 38718.6256 - f3_loss: 12243.1034

# after adding stop_gradient for the conditioning masks from lower-res layers
# stddevs : [0.008, 0.015, 0.03, 0.05]
(120 epochs @ 3E-3) : loss: 44274.7353 - f0_loss: 0.0000e+00 - f1_loss: 36356.2732 - f2_loss: 5704.4272 - f3_loss: 2214.0351

# this sample is problematic (left eye identifies as a lip)
1198061571_1