Average euclidean distance squared b/w pred and truth points. 
Note that all point coordinates (there are 194) were normalized to be b/w [0, 1].

fully_connected_v1.h5 : 0.00011050
fully_connected_v2.h5 (fine-tuned on 1/4 of the dataset) : 0.00011460

fully_connected_sparse_025 : 0.0340840682392
fully_connected_sparse_100 : 0.0321483014775

fully_connected_025 : 0.0415233714881
fully_connected_100 : 

what if more reference points helps convergence?

lip_masker_030_zoomed_v1 train error with 128 depth transposed convs (360 epochs): 17686.7696
lip_masker_030_zoomed_v2 train error with 256 depth transposed convs (240 epochs): 17722.7897

lip_masker_030_zoomed train error for 28x28 (240 epochs): 12859.6017
lip_masker_030_zoomed train error for 56x56 (240 epochs): ~49500

lip_masker_sep_050 train error for 56x56 (240 epochs @ 8E-4): ~92500 

lip_masker_100 train error for 56x56 (340 epochs @ 8E-3): ~90800

when using randomized bboxes in training:
lip_masker_rand_bboxes_100 for 56x56 (480 epochs @ 1E-4): ~91000

point maskers:

losses are put in a list. last one is typically the per-coord softmax loss. 
before that are sigmoid losses. first one has lowest level features (less meaningful).
rcf (70 epochs @ 1E-3) : [164366, 189204, 4863]

# ------------------------------
# Week of February 11:
# ------------------------------
small point masker:
(100 epochs @ 1E-3) : loss: 212788.3571 - f0_loss: 55532.1652 - f1_loss: 60587.4566 - f2_loss: 83933.9861 - f3_loss: 12734.7497

for 14x14 resolution, lowest resolution loss:
    guessing all 0s:
    52170.3648

    fairly decent results:
    ~49900 

# stddevs : [0.008, 0.02, 0.03, 0.05]
(120 epochs @ 3E-3) : loss: 76760.3013 - f0_loss: 0.0000e+00 - f1_loss: 25798.5723 - f2_loss: 38718.6256 - f3_loss: 12243.1034

# after adding stop_gradient for the conditioning masks from lower-res layers
# stddevs : [0.008, 0.015, 0.03, 0.05]
(120 epochs @ 3E-3) : loss: 44274.7353 - f0_loss: 0.0000e+00 - f1_loss: 36356.2732 - f2_loss: 5704.4272 - f3_loss: 2214.0351

# this sample is problematic (left eye identifies as a lip)
1198061571_1

# vanilla point masker
# stddev for 56x56 : 0.02
(180 epochs? @ 5E-3) : loss: 5477.5687 - f0_loss: 5477.5687 - val_loss: 8123.8908 - val_f0_loss: 8123.8908

# fancy point masker

# vanilla point masker, no skip connections
(160 epochs? @ 1E-2) : 142ms/step - loss: 5903.6019 - f0_loss: 5903.6019 - val_loss: 8774.8352 - val_f0_loss: 8774.8352

# removed 256 deep layer, changed kernel size from 3x3 to 7x7 for 14x14 feature map
(260 epochs @ 5E-3) : loss: 5239.6429 - f0_loss: 5239.6429 - val_loss: 7949.9180 - val_f0_loss: 7949.9180
2728400620_1 -- really bad on left side of lip
2139669544_1 -- right part of lip is in the dark, tough one
1037255513_1 -- really bad on left side of lip
2426631349_1 -- epic fail on top side of lip

# same model but trained on x-flipped data
(350 epochs @ 5E-3) : loss: 5301.4107 - f0_loss: 5301.4107 - val_loss: 8046.8749 - val_f0_loss: 8046.8749

# dilated convolutions model @ commit c183f349bbaf1d28ce952ab8e22fd1f9189f58c0
(140 epochs? @ 5E-3) : loss: 5713.1293 - f0_loss: 5713.1293 - val_loss: 7471.6771 - val_f0_loss: 7471.6771

# without the final 16-dilated layer and 1 extra conv layer at 112 resolution
(150 epochs? @ 1E-2) : loss: 5828.1249 - f0_loss: 5828.1249 - val_loss: 7757.0386 - val_f0_loss: 7757.0386

# week of 02/24
# dilated convolutions (same as above), but with std=0.01 instead of 0.02 for point masks
(120 epochs @ 6E-2) :  22s 363ms/step - loss: 2126.1918 - pointMaskDistance: 0.1416 - val_loss: 3091.1788 - val_pointMaskDistance: 0.4272

# std=0.02
(165 epochs @ 6E-2) : loss: 5997.1793 - pointMaskDistance: 0.1183 - val_loss: 6295.4792 - val_pointMaskDistance: 0.1185

# vanilla encoder-decoder module with bilinear upsampling
(150 epochs @ 6E-2) : loss: 6391.2594 - pointMaskDistance: 0.1828 - val_loss: 6587.4440 - val_pointMaskDistance: 0.1731

# with odd dilation rates + intermediate dense convolution layers
(120 epochs @ 0.1) : loss: 6240.8353 - pointMaskDistance: 0.1746 - val_loss: 6316.9873 - val_pointMaskDistance: 0.1335

# with odd dilation rates
(150 epochs @ 6E-2) : loss: 6063.8427 - pointMaskDistance: 0.1019 - val_loss: 6061.2618 - val_pointMaskDistance: 0.1022

# the 'attention' model. Note that it's difficult to get consistent results with this one, possibly because of dead ReLUs.
(150, more likely to be 180 in retrospect, epochs @ 6E-2) : loss: 5651.6807 - pointMaskDistance: 0.0552 - val_loss: 5734.3161 - val_pointMaskDistance: 0.0663

# back to the basic encoder-decoder network, except with dilated convolutions in the middle and more 256-d layers
(180 epochs @ 6E-2) : loss: 5550.8757 - pointMaskDistance: 0.0511 - val_loss: 5776.9667 - val_pointMaskDistance: 0.0793

# attention model with tanh and leaky relu (more consistent results than last time):
(150 epochs @ 6E-2): loss: 5727.0221 - pointMaskDistance: 0.0585 - val_loss: 6130.2450 - val_pointMaskDistance: 0.1074

(180 epochs @ 6E-2): loss: 5665.9413 - pointMaskDistance: 0.0517 - val_loss: 5936.5323 - val_pointMaskDistance: 0.0825

(210 epochs @ 6E-2): loss: 5603.3227 - pointMaskDistance: 0.0459 - val_loss: 6274.1019 - val_pointMaskDistance: 0.1105

# from here on out, using numpy random seed = 0. 
# Results may still be slightly inconsistent because of multithreaded (?) data augmentation / batch generation.

# same model as above, std 0.014
(150 epochs @ 6E-2): loss: 3119.0159 - pointMaskDistance: 0.0573 - val_loss: 3275.3870 - val_pointMaskDistance: 0.0676

# same model but with 2 * num_coords filters at bottleneck layer, std 0.02
(150 epochs @ 6E-2) loss: 5632.9867 - pointMaskDistance: 0.0630 - val_loss: 6072.2349 - val_pointMaskDistance: 0.0921

# encoder-decoder with concatenation instead of add
(150 epochs @ 6E-2): loss: 5619.4844 - pointMaskDistance: 0.0499 - val_loss: 5792.6819 - val_pointMaskDistance: 0.0741

# attention model but with a 256-d layer removed and the other reduced to 64-d

# reduced last decoder layer in depth from 128-d to 64-d. removed one or two 256-d layers in the encoder.
(150 epochs w/ nesterov momentum @ 5E-5) loss: 5692.4031 - pointMaskDistance: 0.0539 - val_loss: 5871.3212 - val_pointMaskDistance: 0.0831

# on in-house dataset, 20% val set size, no data augmentation
(90 epochs w/ same lr as above) : loss: 5891.6495 - pointMaskDistance: 0.0552 - val_loss: 7447.9353 - val_pointMaskDistance: 0.2189

# For below: on in-house dataset, 20% val set size, data augmentation
# 150 epochs with nesterov momentum unless otherwise specified

# concat model
loss: 5887.2887 - pointMaskDistance: 0.0812 - val_loss: 6139.4045 - val_pointMaskDistance: 0.0698

# attention model with 4 * num_coords in bottleneck layer
loss: 6048.8748 - pointMaskDistance: 0.1131 - val_loss: 6214.0658 - val_pointMaskDistance: 0.0977

# attention model with num_coords in bottleneck, but with 256-d layers unlike the previous experiment (128-d)
loss: 6046.4767 - pointMaskDistance: 0.0955 - val_loss: 6231.4051 - val_pointMaskDistance: 0.0895

# same model as above but with added pointMaskDistanceLoss
# 2018-03-07:22:12
loss: 4140 - pointMaskDistance: 0.0666 - val_loss: 4296 - val_pointMaskDistance: 0.0782

# same model as above but with adam @ 6E-2
# 2018-03-07:23:26
loss: 4117 - pointMaskDistance: 0.0930 - val_loss: 4239 - val_pointMaskDistance: 0.0878

# same model as above but with slightly adjusted loss function, sgd momentum as before, and color augmentation

# concat model
# 03-11:12:28
loss: 6022.6358 - pointMaskDistance: 0.1165 - val_loss: 6152.5966 - val_pointMaskDistance: 0.0721

# concat model with mask stddev=0.25
loss: 12588.3353 - pointMaskDistance: 0.0594 - val_loss: 12686.1874 - val_pointMaskDistance:0.0746
